defaults:
  - base_pytorch
  - _self_
  
tasks: [training]
strategy: fsdp # distributed strategy to use, options: ddp, deepspeed_stage_2, fsdp

training:
  lr: 1e-5
  precision: bf16-mixed
  batch_size: 1
  max_epochs: 1000
  max_steps: 10000000
  checkpointing:
    every_n_train_steps: 50
    every_n_epochs: null
    save_weights_only: false
    save_on_exception: false
    save_last: link
  optim:
    accumulate_grad_batches: 2
    gradient_clip_val: null
  data:
      num_workers: 2 # number of CPU threads for data preprocessing.

validation:
  precision: bf16-mixed
  val_every_n_step: 50
  val_every_n_epoch: null
  batch_size: 1
  limit_batch: 4
  data:
    num_workers: 3 # number of CPU threads for data preprocessing, for validation.

test:
  precision: bf16-mixed
  limit_batch: null
  batch_size: 1
  data:
    num_workers: 1 # number of CPU threads for data preprocessing, for test.
